# SQL for SQD Network

* Meeting 2025-04-08, Tbilisi
* Participants: Dzmitry (DK), Dzanis (DaK), Eldar (EG), Janusz (JD), Tobias (TS)
* Topic: Metadata

## Context

The SQL query engine (QE) will provide the capability to query arbitrary data,
there is, thus, no _fixed_ datamodel on which this engine would depend.
To achieve this, the engine will rely on metadata that describe the data
and provide additional information (partitioning and partition stats)
to facilitate query execution.
The purpose of the meeting is to define the content of the metadata
and to outline a possible implementation.

## DuckDB Extenstion

The DuckDB extension will use the SQL _attach_ statement that is used to add external resources
(like other databases or CSV or parquet files) to an RBDMS, e.g.:

```
ATTACH 'https://portal.sqd.dev/sql' AS sqd;
SELECT number, hash, timestamp FROM sqd.block
 WHERE number BETWEEN 1234567 AND 2345678;
```

To enable DuckDB to use external resources the extension shall feed it with information about these resources.
Internally it requests metadata from the SQD server and presents it to DuckDB.
The [Postgres extension](https://duckdb.org/docs/stable/extensions/postgres.html)
for DuckDB is a starting point to learn how to do that.

## Metadata

The metadata reside in the server and are provided to the client through an API.
When ``ATTACH`` is called in DuckDB, the extension requests the metadata
through the API and makes it available to the local DuckDB QE.
Since the SQL QE will implement only **DQL** capabilities
data creation and ingestion are outside of the scope of this engine.
Services that actually create datamodels
and append data to it will also provide descriptions of that datamodel as well as data statistics
and store them in the metadata facility.

### Content

Conceptually, metadata can be described as an arry of dataset descriptors.
Dataset descriptors consist of 

* a name 
* potentially other information
* table descriptors.

A table descriptor has the following form (here presented in JSON format):

```
{
"name": "some-table-name",
"schema": {
    "fields": [...],
    "partitionKeys": [...],
    "chunks": [...],
}
```

Fields represent table columns and contain at least the following information:

* column name
* column type
* nullability

The list of fields needs to reflect changes that occur during the lifetime of the schema.
That means that a field may appear more than once in the column list but with different attributes
(i.e. type or nullability changed).

Paritions are defined by key ranges.
More than one column per table may serve as partition criterion, e.g.,
we may partition per block number **and** per timestamp.

While partition keys _define_ the partitioning criteria, chunks represent the actual partitions.
The chunk object contains at least

* the parquet files of this chunk as list of alternative URLs
* additional stats (number of rows, min/max key, Bloom filters over keys, etc.)

### Implementation

A promising candidate for implementing the metadata is [IPLD](https://ipld.io/),
a content-addressable data model. IPLD defines trees of documents (in a "JSON-like" structure)
that use cryptographic hashes for identification together with _codecs_ for (de)serialisation
(see [CID](https://github.com/multiformats/cid), a self-describing content identifier).
IPLD datamodels provide interesting properties for our use case:

* They support a decentralised approach (hashes instead of URLs to identify and reference documents)
* They can be easily validated based on their hashes
* Updates can be (recursively) detected by means of hashes
* Which, in turn, facilitates caching.
* Codecs, which are part of document identification, allow for unambiguous (de)serialisation.

IPLD further provides _paths_ and _selectors_ that allow navigating a document tree, either by
stepping to known sub-paths or by generic traversal using selectors. Paths and selectors are similar
to _XPATH_ and _XQUERY_ expressions.

Any process that affects the metadata (by applying changes to the data definition or by data ingestion)
needs to update subtrees of the metadata. IPLD is "immutable by default": any change to a document
implies creating a copy which, after the change, will have a new hash
and, hence, a new content identifier. The process repeats when the changed subdocument is inserted
into the parent document: a new copy with a new content identifier for the parent is created.
Concurrency is therefore easy to implement: a single compare-and-swap operation on the top-level document
is sufficient to detect conurrent changes which would then require recursive re-application of the changes.

When the client requests access to the metadata as part of the _attach_ mechanism,
the document is serialised to plain JSON and sent back to the requesting client.

#### Hot Blocks

Hot blocks are added to an _incomplete_ parquet file that grows until the blocks are properly ingested.

Questions (TS):

* Or is the parquet file simply _closed_ when block numbers outgrow the partition key?
* Is there a special process handling hot blocks or are they just part of the normal ingestion flow?

## Next Steps

* Implement the metadata management in the portal (EG, DK, TS)
* Continue the DuckDB extension (TS), in particular,

    * The _attach_ mechanism (using fake metadata)
    * Plan extraction and manipulation.
